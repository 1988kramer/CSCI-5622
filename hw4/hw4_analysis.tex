\documentclass[11pt]{article}

% ==== PACKAGES ==== %
% \usepackage{fullpage}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{epic}
\usepackage{eepic}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage[letterpaper, margin=1in]{geometry}

% ==== MARGINS ==== %
% \pagestyle{empty}
% \setlength{\oddsidemargin}{0in}
% \setlength{\textwidth}{6.8in}
% \setlength{\textheight}{9.5in}

\pagestyle{fancy}
\fancyhf{}
\rhead{CSCI 5622}
\lhead{Homework 1}
\rfoot{Page \thepage}


\newtheorem*{solution*}{Solution}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{corollary}[lemma]{Corollary}
\lstset{moredelim=[is][\bfseries]{[*}{*]}}

% ==== DOCUMENT PROPER ==== %
\begin{document}

\thispagestyle{empty}

% --- Header Box --- %
\newlength{\boxlength}\setlength{\boxlength}{\textwidth}
\addtolength{\boxlength}{-4mm}

\begin{center}\framebox{\parbox{\boxlength}{\bf
      Machine Learning \hfill Homework 1\\
      CSCI 5622 Fall 2017 \hfill Due Time Nov 10, 2017\\
      Name: Andrew Kramer \hfill CU identitykey: ankr1041
}}
\end{center}




\section{Support Vector Machines (40pts)}

\subsection{Comment on the experiments you ran with GridSearch and the optimal hyperparameters you found}

\subparagraph{}

A linear kernel was tested with penalty parameters of 1, 10, 100, and 1000. The highest accuracy for the linear kernel was obtained with a penalty parameter of 1, but overall the maximum accuracy of 0.964 obtained by the linear kernel was not competitive with the RBF and polynomial kernels.

\begin{figure}[H]
\centering
	\includegraphics[width=0.6\linewidth]{linear.png}
	\caption{linear kernel results}
\end{figure}

An RBF kernel was tested with penalty parameters of 1, 10, 100, and 1000 and gamma values of 0.0001 and 0.001. The maximum accuracy of 0.985 was obtained with a penalty parameter of 1000 and gamma of 0.001. 

\begin{figure}[H]
\centering
	\includegraphics[width=0.6\linewidth]{RBF.png}
	\caption{RBF kernel results}
\end{figure}

A polynomial kernel was tested with degree of 2, 3, 4, and 5 with penalty parameters of 1, 10, 100, and 1000. The maximum accuracy of 0.991 was obtained with a degree of 2 and penalty parameter of 1000. See the graphs below for details.

\begin{figure}[H]
\centering
	\includegraphics[width=0.6\linewidth]{poly.png}
	\caption{polynomial kernel results}
\end{figure}

\subsection{Comment on classification performance for each model for optimal parameters by either testing on a hold-out set or performing cross-validation.}

\subparagraph{}

The highest performing configurations found in the previous section were trained and tested on separate datasets. The RBF and polynomial kernels achieved 0.99 testing accuracy for both the 4 and 9 classes. The linear kernel achieved a testing accuracy of 0.97 for 4s, 0.98 for 9s, and 0.97 average.

\subsection{Give examples in picture form of support vectors from each class when using a polynomial kernel.}

\subparagraph{}

The following examples were obtained by randomly sampling from the set of support vectors found when training a classifier with a 2nd degree polynomial kernel and penalty parameter of 1000.

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.2\linewidth}
		\includegraphics[width=\linewidth]{mnistfig8.png}
		\caption{A nine}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.2\linewidth}
		\includegraphics[width=\linewidth]{mnistfig514.png}
		\caption{A second nine}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.2\linewidth}
		\includegraphics[width=\linewidth]{mnistfig388.png}
		\caption{A four}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.2\linewidth}
		\includegraphics[width=\linewidth]{mnistfig199.png}
		\caption{Another four}
	\end{subfigure}
\end{figure}

\section{Learnability (25pts)}

\subsection{Give a bound on the number of randomly drawn training examples sufficient to assure that for any target class c in C, any consistent learner will output a hypothesis with error at most 0.15 with probability 0.95}

\begin{align*}
	m &\geq \frac{1}{\epsilon}\bigg(\ln \vert H \vert + \ln\frac{1}{\delta}\bigg)\\
	\\
	\delta &= 1-0.95 = 0.05\\
	\epsilon &= 0.15\\
	\\
	\vert H \vert &= \text{number of possible triangles in the space}\\
	\vert H \vert &= {{\text{possible vertices}}\choose{3}}\\
	\text{possible vertices} &= 100^2 = 10000\\
	\vert H \vert &= {{10000}\choose{3}}=166616670000\\
	\\
	m &\geq \frac{1}{.15}\bigg(\ln(166616670000) + \ln\frac{1}{0.05}\bigg)\\
	m &\geq 192\\
\end{align*}

\section{VC Dimension (20pts)}

\subsection{Consider the class of hypotheses defined by circles centered at the origin. A hypothesis $h$ in this class can either classify points as positive if they lie on the boundary or interior of the circle, or if they lie on the boundary or exterior of the circle. State and rigorously prove the VC dimension of this family of classifiers.}
\begin{quote}

	\vspace{3 mm}

	The VC dimension for origin-centered circles is 2.
	
	\vspace{1 mm}

	\textbf{Lower bound:} Consider a set of 2 points with radii $r_1 < r_2$ from the origin. There exists a hypothesis $h$ that can label the points at $r_1$ positive while $r_2$ is labeled negative or vice versa. A hypothesis also exists that can label both points positive or both points negative. So there exists a set of 2 points that can be shattered by an origin-centered circle. 
	
	\vspace{1 mm}

	\textbf{Upper bound:} With a set of 3 points at radii $r_1 \leq r_2 \leq r_3$ from the origin no hypothesis $h$ will be able to label $r_1$ and $r_3$ as positive while also labeling $r_2$ negative. Thus, no set of three points can be shatered by an origin-centered circle.
\end{quote}

\section{Extra Credit (10pts)}

\subsection{Consider the class of hypotheses defined by circles anywhere in 2D space. A hypothesis $h$ in this class can either classify points as positive if they lie on the boundary or interior of the circle, or if they lie on the boundary or exterior of the circle. State and rigorously prove the VC dimension if this family of classifiers.}
\begin{quote}
	
	\vspace{3 mm}

	The VC dimension for circles in a 2D plane is 3.

	\vspace{1 mm}
	
	\textbf{Lower bound:} Consider 3 points that make up a non-degenerate triangle. A circle centered anywhere in the space can shatter these three points.

	\vspace{1 mm}

	\textbf{Upper bound:} If there are 4 points that must be labeled the following cases are possible. If the fourth point is inside the convex hull of the other 3 then it is not possible to label the inside points as -1 and the outside points as +1 (or vice versa). In the second case, the 4 points form a convex polygon. It is not possible for one circle to label these points +1, -1, +1, -1 and another circle to label the same points -1, +1, -1, +1. 
	
	\vspace{1 mm}
	
	Also, in any case a circle in 2D space can be seen as a 2-dimensional classifier. The two dimensions being the radius of the circle and the distance between the origin and the center of the circle. By Radon's theorem, for a d-dimensional classifier there exists a set of $d+1$ points such that every two nonempty subsets can be seperated from each other by a hyperplane. However, no matter which set of $d+2$ points is given, the two subsets of a Radon partition cannot be linearly separated. Therefore the VC dimension of this class of hypotheses is $d+1$, or 3 in this case.
\end{quote}

\end{document}
